{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WHD3uKpLIrlt"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import collections #used to count frequency of elements in a list\n",
        "from eda_functions import *\n",
        "#displays the output inline\n",
        "#%matplotlib inline\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "#modeling\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import\n",
        "\n",
        "df = pd.read_csv(\"Bias_correction_ucl.csv\")\n",
        "df.head()\n",
        "\n",
        "\n",
        "\n",
        "df_dimensions(df)\n",
        "cols_missing_values(df)\n",
        "\n",
        "\n",
        "#Checking if there is a large percentage of missing data\n",
        "most_missing_values(df, .75)\n",
        "\n",
        "\n",
        "\n",
        "#Checking if there is a large percentage of missing data\n",
        "most_missing_values(df, .5)\n",
        "\n",
        "\n",
        "\n",
        "#Checking in detail the percentage of missing data\n",
        "df.isnull().mean()*100\n",
        "\n",
        "df = df.dropna(axis=0)\n",
        "df_dimensions(df)\n",
        "\n",
        "\n",
        "\n",
        "#Checking in detail the percentage of missing data\n",
        "df.isnull().mean()*100\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Calculating the percentage of data removed after droping rows with missing data\n",
        "#For this purpose the result of the function df_dimensions() was used before and after dropping the rows with missing data\n",
        "(7752 -7588)/7752 *100\n",
        "\n",
        "categorical_cols(df)\n",
        "\n",
        "date = df.Date.value_counts()\n",
        "(date/df.shape[0]).plot(kind=\"bar\");\n",
        "plt.title(\"Date\");\n",
        "\n",
        "len(df.Date.value_counts())\n",
        "\n",
        "\n",
        "#Checking the date format to see how we are going to separate year, month, and day info\n",
        "df.Date[0]\n",
        "\n",
        "\n",
        "df.Date[0][:4] #year\n",
        "\n",
        "\n",
        "df.Date[0][5:7] #month\n",
        "\n",
        "\n",
        "df.Date[0][8:] #day\n",
        "\n",
        "df[\"year\"] = df[\"Date\"].apply(lambda x: x[:4]) #get the year from the date\n",
        "df[\"month\"] = df[\"Date\"].apply(lambda x: x[5:7]) #get the month from the date\n",
        "df[\"day\"] = df[\"Date\"].apply(lambda x: x[8:]) #get the day from the date\n",
        "df[[\"year\", \"month\", \"day\"]]\n",
        "\n",
        "df.year.value_counts()\n",
        "\n",
        "\n",
        "years = [\"2013\", \"2014\", \"2015\", \"2016\", \"2017\"]\n",
        "for y in years:\n",
        "    check_data_month(df, y)\n",
        "    print(\"______________ \\n\")\n",
        "\n",
        "\n",
        "\n",
        "df.groupby('month').year.value_counts().unstack(0).plot.bar()\n",
        "plt.ylabel('Total number of registers per month')\n",
        "plt.title(\"Total number of registers\");\n",
        "\n",
        "\n",
        "#Analyzing date info for every year and every month\n",
        "months = [\"06\", \"07\", \"08\"]\n",
        "for y in years:\n",
        "    print(\"Year: \", y)\n",
        "    print(\"- - - - - - -\")\n",
        "    for m in months:\n",
        "        print(\"Month: \", m)\n",
        "        check_data_day(df, y, m)\n",
        "        print(\"\\n\")\n",
        "    print(\"______________ \\n\")\n",
        "\n",
        "\n",
        "\n",
        "numerical_cols(df)\n",
        "\n",
        "\n",
        "df.describe()\n",
        "\n",
        "df.hist(figsize = (14,12));\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(15,15)) \n",
        "sns.heatmap(df.corr(), annot=True, fmt=\".2f\")\n",
        "\n",
        "\n",
        "important_columns = [\"Next_Tmax\", \"Next_Tmin\"]\n",
        "for col in important_columns:\n",
        "    year_ditribution(df,col)\n",
        "\n",
        "for y in years:\n",
        "    month_distribution(df,\"Next_Tmax\" , y)\n",
        "\n",
        "df.plot(x=\"lon\", y=\"lat\", kind=\"scatter\", c=\"Next_Tmax\", colormap=\"YlOrRd\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "#Droping unecessary columns\n",
        "df.drop(columns= [\"station\", \"Date\"], inplace=True)\n",
        "df.columns\n",
        "\n",
        "#Transforming date columns from string to int\n",
        "columns_date = [\"year\", \"month\", \"day\"]\n",
        "for col in columns_date:\n",
        "    df = string_into_int(df, col)\n",
        "\n",
        "#Split data in X and y\n",
        "X = X = df.drop([\"Next_Tmax\", \"Next_Tmin\"], axis='columns')\n",
        "y_max = df[\"Next_Tmax\"]\n",
        "y_min = df[\"Next_Tmin\"]\n",
        "\n",
        "print(X.shape)\n",
        "print(y_max.shape)\n",
        "print(y_min.shape)\n",
        "\n",
        "#Split into train and test\n",
        "X_train_max, X_test_max, y_train_max, y_test_max = train_test_split(X, y_max, test_size=.30, random_state=42)\n",
        "X_train_min, X_test_min, y_train_min, y_test_min = train_test_split(X, y_min, test_size=.30, random_state=42)\n",
        "\n",
        "#Instantiate model\n",
        "lm_model_max = LinearRegression(normalize=True)\n",
        "lm_model_min = LinearRegression(normalize=True)\n",
        "\n",
        "#Fit\n",
        "lm_model_max.fit(X_train_max, y_train_max)\n",
        "lm_model_min.fit(X_train_min, y_train_min)\n",
        "\n",
        "#Predict and score the model\n",
        "y_test_preds_max = lm_model_max.predict(X_test_max)\n",
        "y_test_preds_min = lm_model_min.predict(X_test_min)\n",
        "\n",
        "rsquared_score_max = r2_score(y_test_max, y_test_preds_max)\n",
        "rsquared_score_min = r2_score(y_test_min, y_test_preds_min)\n",
        "\n",
        "\n",
        "#Rsquared and y_test\n",
        "length_y_test_max = len(y_test_preds_max)\n",
        "length_y_test_min = len(y_test_preds_min)\n",
        "print(\"The r-squared score for your model_max was {} on {} values.\".format(rsquared_score_max, length_y_test_max))\n",
        "print(\"The r-squared score for your model_min was {} on {} values.\".format(rsquared_score_min, length_y_test_min))\n",
        "\n",
        "#To check which features matter in the model, check the weight of the coeficients\n",
        "#Because the features are normalized we can look at how large the coeficient is\n",
        "coef_df_max = coef_weights(lm_model_max.coef_, X_train_max, lm_model_max)\n",
        "coef_df_max.head(20)\n",
        "\n",
        "coef_df_min = coef_weights(lm_model_min.coef_, X_train_min, lm_model_min)\n",
        "coef_df_min.head(20)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
